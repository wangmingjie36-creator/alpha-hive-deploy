"""
ğŸ Alpha Hive - æŠ•èµ„ç»„åˆé›†ä¸­åº¦åˆ†æ (P4)
æ‰«æå®Œæˆåæ£€æµ‹é«˜åˆ†æ ‡çš„çš„æ¿å—/å› å­é‡å é£é™©

åŠŸèƒ½ï¼š
- 30 æ—¥ä»·æ ¼ç›¸å…³æ€§çŸ©é˜µï¼ˆ>0.7 = é«˜åº¦ç›¸å…³è­¦å‘Šï¼‰
- æ¿å—é›†ä¸­åº¦æ£€æµ‹ï¼ˆ>60% åŒä¸€æ¿å— = é£é™©ï¼‰
- å› å­æš´éœ²åˆ†æï¼ˆåŠ¨é‡/æ³¢åŠ¨ç‡èšç±»ï¼‰
- ç»™å‡ºåˆ†æ•£åŒ–å»ºè®®

å…è´¹æ•°æ®ï¼šyfinance å†å²ä»·æ ¼ + config.WATCHLIST æ¿å—ä¿¡æ¯
"""

import logging
import math
from typing import Dict, List, Optional

_log = logging.getLogger("alpha_hive.portfolio_concentration")


def analyze_concentration(
    swarm_results: Dict,
    watchlist: Dict,
    threshold_score: float = 6.0,
    top_n: int = 8,
) -> Dict:
    """
    åˆ†æé«˜åˆ†æ ‡çš„çš„é›†ä¸­åº¦é£é™©

    Args:
        swarm_results: run_swarm_scan() è¿”å›çš„èœ‚ç¾¤ç»“æœ dict
        watchlist: config.WATCHLIST
        threshold_score: æœ€ä½åˆ†æ•°è¿‡æ»¤çº¿
        top_n: æœ€å¤šåˆ†æå‡ ä¸ªæ ‡çš„

    Returns:
        {
            "concentration_risk": "low"/"medium"/"high",
            "risk_score": float (0-10),
            "sector_breakdown": dict,
            "correlation_warnings": list,
            "factor_clusters": dict,
            "recommendations": list[str],
            "summary": str,
        }
    """
    # è¿‡æ»¤é«˜åˆ†æ ‡çš„
    high_score = {
        t: d for t, d in swarm_results.items()
        if d.get("final_score", 0) >= threshold_score
    }
    if not high_score:
        return {
            "concentration_risk": "low",
            "risk_score": 0.0,
            "sector_breakdown": {},
            "correlation_warnings": [],
            "factor_clusters": {},
            "recommendations": ["æš‚æ— é«˜åˆ†æ ‡çš„ï¼Œæ— éœ€è¯„ä¼°é›†ä¸­åº¦"],
            "summary": "æ— é«˜åˆ†æ ‡çš„ï¼ˆåˆ†æ•° < 6.0ï¼‰",
        }

    tickers = list(high_score.keys())[:top_n]

    # ---- 1. æ¿å—é›†ä¸­åº¦ ----
    sector_map = {}
    for t in tickers:
        sector = watchlist.get(t, {}).get("sector", "Unknown")
        sector_map.setdefault(sector, []).append(t)

    total = len(tickers)
    sector_breakdown = {s: {"tickers": ts, "pct": round(len(ts)/total*100, 1)}
                        for s, ts in sector_map.items()}

    max_sector_pct = max((v["pct"] for v in sector_breakdown.values()), default=0)
    max_sector = max(sector_breakdown, key=lambda s: sector_breakdown[s]["pct"], default="")

    # ---- 2. ä»·æ ¼ç›¸å…³æ€§çŸ©é˜µï¼ˆä½¿ç”¨ yfinance 30æ—¥æ”¶ç›Šç‡ï¼‰----
    correlation_warnings = []
    returns_map = {}

    try:
        import yfinance as yf
        data = yf.download(tickers, period="1mo", interval="1d",
                           auto_adjust=True, progress=False)
        if not data.empty:
            # pandas DataFame: columns may be MultiIndex or single
            if hasattr(data.columns, "levels"):
                close = data["Close"] if "Close" in data.columns.get_level_values(0) else data
            else:
                close = data

            pct_returns = close.pct_change().dropna()

            for i, t1 in enumerate(tickers):
                for t2 in tickers[i+1:]:
                    if t1 in pct_returns.columns and t2 in pct_returns.columns:
                        s1 = pct_returns[t1].dropna()
                        s2 = pct_returns[t2].dropna()
                        common = s1.index.intersection(s2.index)
                        if len(common) < 5:
                            continue
                        v1 = list(s1[common])
                        v2 = list(s2[common])
                        corr = _pearson(v1, v2)
                        if corr is not None and corr >= 0.70:
                            correlation_warnings.append({
                                "pair": f"{t1}/{t2}",
                                "correlation": round(corr, 2),
                                "risk": "high" if corr >= 0.85 else "medium",
                            })

            # æå–åŠ¨é‡æ•°æ®ç”¨äºå› å­èšç±»
            for t in tickers:
                if t in pct_returns.columns:
                    series = pct_returns[t].dropna()
                    if len(series) >= 5:
                        returns_map[t] = {
                            "momentum_30d": round(float(series.sum() * 100), 2),
                            "volatility": round(float(series.std() * (252**0.5) * 100), 2),
                        }
    except Exception as e:
        _log.debug("ä»·æ ¼ç›¸å…³æ€§è®¡ç®—å¤±è´¥: %s", e)

    # ---- 3. å› å­èšç±»ï¼ˆç®€å•åˆ†ç»„ï¼šé«˜åŠ¨é‡ / ä½åŠ¨é‡ / é«˜æ³¢åŠ¨ï¼‰----
    factor_clusters = {"é«˜åŠ¨é‡(>5%)": [], "ä½åŠ¨é‡(<0%)": [], "é«˜æ³¢åŠ¨(>60%å¹´åŒ–)": [], "ç¨³å®š": []}
    for t, r in returns_map.items():
        mom = r.get("momentum_30d", 0)
        vol = r.get("volatility", 0)
        if vol > 60:
            factor_clusters["é«˜æ³¢åŠ¨(>60%å¹´åŒ–)"].append(t)
        elif mom > 5:
            factor_clusters["é«˜åŠ¨é‡(>5%)"].append(t)
        elif mom < 0:
            factor_clusters["ä½åŠ¨é‡(<0%)"].append(t)
        else:
            factor_clusters["ç¨³å®š"].append(t)
    factor_clusters = {k: v for k, v in factor_clusters.items() if v}

    # ---- 4. ç»¼åˆé£é™©è¯„åˆ† ----
    risk_score = 0.0

    # æ¿å—é›†ä¸­åº¦è´¡çŒ®ï¼ˆ0-5 åˆ†ï¼‰
    if max_sector_pct >= 70:
        risk_score += 5.0
    elif max_sector_pct >= 50:
        risk_score += 3.0
    elif max_sector_pct >= 35:
        risk_score += 1.5

    # ç›¸å…³æ€§è­¦å‘Šè´¡çŒ®ï¼ˆæ¯å¯¹é«˜ç›¸å…³ +1ï¼Œæœ€å¤š 5 åˆ†ï¼‰
    high_corr = [w for w in correlation_warnings if w["risk"] == "high"]
    risk_score += min(5.0, len(high_corr) * 1.5 + len(correlation_warnings) * 0.5)
    risk_score = min(10.0, risk_score)

    if risk_score >= 7:
        concentration_risk = "high"
    elif risk_score >= 4:
        concentration_risk = "medium"
    else:
        concentration_risk = "low"

    # ---- 5. å»ºè®® ----
    recommendations = []
    if max_sector_pct >= 60:
        others = [s for s in sector_breakdown if s != max_sector]
        recommendations.append(
            f"âš  {max_sector}æ¿å—å  {max_sector_pct:.0f}%ï¼ˆ{sector_map[max_sector]}ï¼‰ï¼Œå»ºè®®è¡¥å……{'/'.join(others[:2]) or 'å…¶ä»–æ¿å—'}æ ‡çš„"
        )
    if high_corr:
        pairs = [w["pair"] for w in high_corr[:3]]
        recommendations.append(
            f"âš  é«˜ç›¸å…³å¯¹ï¼š{', '.join(pairs)}ï¼ˆç›¸å…³ç³»æ•°â‰¥0.85ï¼‰ï¼Œå®é™…é£é™©æ•å£å¯èƒ½å°‘äºè¡¨é¢æ ‡çš„æ•°é‡"
        )
    if len(factor_clusters.get("é«˜æ³¢åŠ¨(>60%å¹´åŒ–)", [])) >= 3:
        recommendations.append("âš  å¤šä¸ªé«˜æ³¢åŠ¨æ ‡çš„åŒæ—¶é«˜åˆ†ï¼Œå»ºè®®æ§åˆ¶å•ç¥¨ä»“ä½ä¸Šé™")
    if not recommendations:
        recommendations.append("âœ… æ¿å—åˆ†å¸ƒåˆç†ï¼Œç›¸å…³æ€§é£é™©å¯æ§")

    summary_parts = [f"{total}ä¸ªé«˜åˆ†æ ‡çš„"]
    if max_sector:
        summary_parts.append(f"{max_sector}å {max_sector_pct:.0f}%")
    if correlation_warnings:
        summary_parts.append(f"{len(correlation_warnings)}å¯¹é«˜ç›¸å…³")
    summary = " | ".join(summary_parts)

    return {
        "concentration_risk": concentration_risk,
        "risk_score": round(risk_score, 1),
        "sector_breakdown": sector_breakdown,
        "correlation_warnings": sorted(correlation_warnings,
                                       key=lambda x: x["correlation"], reverse=True)[:6],
        "factor_clusters": factor_clusters,
        "recommendations": recommendations,
        "summary": summary,
        "tickers_analyzed": tickers,
        "returns_data": returns_map,
    }


def _pearson(x: List[float], y: List[float]) -> Optional[float]:
    """è®¡ç®—çš®å°”æ£®ç›¸å…³ç³»æ•°"""
    n = len(x)
    if n < 3:
        return None
    try:
        mx = sum(x) / n
        my = sum(y) / n
        num = sum((xi - mx) * (yi - my) for xi, yi in zip(x, y))
        dx = math.sqrt(sum((xi - mx)**2 for xi in x))
        dy = math.sqrt(sum((yi - my)**2 for yi in y))
        if dx * dy == 0:
            return None
        return num / (dx * dy)
    except (ZeroDivisionError, ValueError):
        return None
