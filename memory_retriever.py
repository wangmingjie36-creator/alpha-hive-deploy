#!/usr/bin/env python3
"""
ğŸ” Alpha Hive Memory Retriever - è·¨ä¼šè¯è®°å¿†æ£€ç´¢å¼•æ“
åŸºäº TF-IDF çš„ä¸­è‹±æ··åˆåˆ†è¯ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œ< 50ms æ€§èƒ½ç›®æ ‡
"""

import logging as _logging
import re
import json
import time
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from datetime import datetime, timedelta
from threading import Lock

_log = _logging.getLogger("alpha_hive.memory_retriever")

try:
    import numpy as np
except ImportError:
    np = None


class MemoryRetriever:
    """åŸºäº TF-IDF çš„è®°å¿†æ£€ç´¢å¼•æ“ï¼ˆå« LRU ç¼“å­˜ç®¡ç†ï¼‰"""

    # ç¼“å­˜ä¸Šé™ï¼šé˜²æ­¢æ— é™å¢é•¿å¯¼è‡´å†…å­˜æ³„æ¼
    MAX_CACHE_TICKERS = 50       # æœ€å¤šç¼“å­˜ 50 ä¸ª ticker çš„æ–‡æ¡£
    MAX_TFIDF_CACHE = 30         # æœ€å¤šç¼“å­˜ 30 ä¸ª ticker çš„ TF-IDF å‘é‡
    MAX_CONTEXT_CHARS = 200      # Agent æ³¨å…¥çš„ä¸Šä¸‹æ–‡æ‘˜è¦æœ€å¤§å­—ç¬¦æ•°

    def __init__(self, memory_store, cache_ttl_seconds: int = 300):
        """
        åˆå§‹åŒ–æ£€ç´¢å¼•æ“

        Args:
            memory_store: MemoryStore å®ä¾‹
            cache_ttl_seconds: ç¼“å­˜ TTLï¼ˆç§’ï¼‰
        """
        self.memory_store = memory_store
        self.cache_ttl_seconds = cache_ttl_seconds

        # ç¼“å­˜ï¼š{ticker: {"timestamp": float, "documents": List[Dict]}}
        self._cache: Dict[str, Dict] = {}
        self._cache_lock = Lock()

        # TF-IDF ç¼“å­˜ï¼š{ticker: {"idf": Dict, "vocab": Dict}}
        self._tfidf_cache: Dict[str, Dict] = {}

    def _evict_lru_cache(self) -> None:
        """LRU æ·˜æ±°ï¼šå½“ç¼“å­˜è¶…è¿‡ä¸Šé™æ—¶ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®"""
        with self._cache_lock:
            if len(self._cache) > self.MAX_CACHE_TICKERS:
                # æŒ‰ timestamp æ’åºï¼Œæ·˜æ±°æœ€æ—§çš„
                sorted_keys = sorted(
                    self._cache.keys(),
                    key=lambda k: self._cache[k].get("timestamp", 0)
                )
                # æ·˜æ±°è¶…å‡ºéƒ¨åˆ†
                for key in sorted_keys[:len(self._cache) - self.MAX_CACHE_TICKERS]:
                    del self._cache[key]

            if len(self._tfidf_cache) > self.MAX_TFIDF_CACHE:
                # TF-IDF ç¼“å­˜æ²¡æœ‰ timestampï¼Œç›´æ¥æ·˜æ±°å‰ N ä¸ª
                keys_to_remove = list(self._tfidf_cache.keys())[:-self.MAX_TFIDF_CACHE]
                for key in keys_to_remove:
                    del self._tfidf_cache[key]

    def _tokenize(self, text: str) -> List[str]:
        """
        ä¸­è‹±æ··åˆåˆ†è¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸ä¾èµ– jiebaï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            è¯åˆ—è¡¨
        """
        # æ¸…ç†æ–‡æœ¬
        text = text.lower().strip()

        # åˆ†ç¦»ä¸­æ–‡å’Œè‹±æ–‡
        tokens = []

        # è‹±æ–‡åˆ†è¯ï¼šæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åˆ†å‰²
        parts = re.split(r'[\s\-_.,!?;:]+', text)
        for part in parts:
            if part:
                tokens.append(part)

        # æå–ä¸­æ–‡å­—ç¬¦ï¼ˆç®€åŒ–ï¼šæ¯ä¸ªä¸­æ–‡å­—ç¬¦ä¸€ä¸ªè¯ï¼‰
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        tokens.extend(chinese_chars)

        # è¿‡æ»¤åœç”¨è¯å’Œè¿‡çŸ­è¯
        stopwords = {'çš„', 'æ˜¯', 'å’Œ', 'æˆ–', 'å¦‚', 'ä½†', 'a', 'an', 'the', 'is', 'and', 'or'}
        tokens = [t for t in tokens if len(t) > 1 or t in 'äº®å¤šç©ºæ¶¨è·Œ']

        return tokens

    def _build_tfidf(self, documents: List[Dict]) -> Tuple[Dict, Dict]:
        """
        æ„å»º TF-IDF å‘é‡

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« 'discovery' å’Œ 'source' å­—æ®µ

        Returns:
            (idf_dict, vocab_dict)
        """
        if not documents:
            return {}, {}

        # 1. åˆ†è¯
        all_tokens = []
        doc_tokens_list = []

        for doc in documents:
            text = doc.get('discovery', '') + ' ' + doc.get('source', '')
            tokens = self._tokenize(text)
            doc_tokens_list.append(set(tokens))
            all_tokens.extend(tokens)

        # 2. è®¡ç®— IDF
        vocab = list(set(all_tokens))
        doc_count = len(documents)
        idf = {}

        for word in vocab:
            doc_freq = sum(1 for doc_tokens in doc_tokens_list if word in doc_tokens)
            idf[word] = np.log((doc_count + 1) / (doc_freq + 1)) if np else 1.0

        return idf, {word: i for i, word in enumerate(vocab)}

    def _compute_similarity(self, query: str, doc: Dict, idf: Dict, vocab: Dict) -> float:
        """
        è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            doc: æ–‡æ¡£å­—å…¸
            idf: IDF å­—å…¸
            vocab: è¯è¡¨

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° [0, 1]
        """
        if not vocab or not idf:
            return 0.0

        # åˆ†è¯
        query_tokens = self._tokenize(query)
        doc_tokens = self._tokenize(doc.get('discovery', '') + ' ' + doc.get('source', ''))

        # è®¡ç®—è¯é¢‘å‘é‡
        query_vec = defaultdict(float)
        doc_vec = defaultdict(float)

        for word in query_tokens:
            if word in idf:
                query_vec[word] += 1.0

        for word in doc_tokens:
            if word in idf:
                doc_vec[word] += 1.0

        # åº”ç”¨ IDF åŠ æƒ
        for word in query_vec:
            query_vec[word] *= idf.get(word, 1.0)

        for word in doc_vec:
            doc_vec[word] *= idf.get(word, 1.0)

        # ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = sum(query_vec[w] * doc_vec[w] for w in query_vec if w in doc_vec)

        query_norm = np.sqrt(sum(v ** 2 for v in query_vec.values())) if np else 1.0
        doc_norm = np.sqrt(sum(v ** 2 for v in doc_vec.values())) if np else 1.0

        if query_norm == 0 or doc_norm == 0:
            return 0.0

        return dot_product / (query_norm * doc_norm)

    def find_similar(
        self,
        query: str,
        ticker: Optional[str] = None,
        top_k: int = 5,
        min_similarity: float = 0.1
    ) -> List[Dict]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼çš„å†å²è®°å¿†

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆè‡ªç„¶è¯­è¨€æˆ–å…³é”®è¯ï¼‰
            ticker: å¯é€‰çš„è‚¡ç¥¨è¿‡æ»¤
            top_k: è¿”å›ç»“æœæ•°é‡
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« 'similarity' å­—æ®µ
        """
        try:
            # LRU æ·˜æ±°æ£€æŸ¥
            self._evict_lru_cache()

            # è·å–æœ€è¿‘è®°å¿†ï¼ˆ30 å¤©å†…ï¼Œé™åˆ¶ 50 æ¡é˜²æ­¢å†…å­˜è†¨èƒ€ï¼‰
            if ticker:
                memories = self.memory_store.get_recent_memories(ticker, days=30, limit=50)
            else:
                return []

            if not memories:
                return []

            # æ„å»º TF-IDFï¼ˆç¼“å­˜å‘½ä¸­æ—¶è·³è¿‡é‡å»ºï¼‰
            idf, vocab = self._build_tfidf(memories)

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = []
            for doc in memories:
                sim = self._compute_similarity(query, doc, idf, vocab)
                if sim >= min_similarity:
                    similarities.append({
                        'memory_id': doc.get('memory_id'),
                        'ticker': doc.get('ticker'),
                        'agent_id': doc.get('agent_id'),
                        'discovery': doc.get('discovery'),
                        'direction': doc.get('direction'),
                        'self_score': doc.get('self_score'),
                        'source': doc.get('source'),
                        'created_at': doc.get('created_at'),
                        'similarity': round(sim, 3)
                    })

            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similarities.sort(key=lambda x: x['similarity'], reverse=True)

            return similarities[:top_k]

        except (ValueError, KeyError, TypeError, AttributeError) as e:
            _log.error("find_similar å¤±è´¥: %s", e, exc_info=True)
            return []

    def get_context_summary(self, ticker: str, current_date: str, days: int = 30) -> str:
        """
        è·å–å†å²ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆç”¨äº Agent æ³¨å…¥ï¼‰

        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            current_date: å½“å‰æ—¥æœŸ
            days: å›æº¯å¤©æ•°

        Returns:
            å†å²æ‘˜è¦å­—ç¬¦ä¸²ï¼ˆå¦‚æœæ— å†å²ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼‰
        """
        try:
            memories = self.memory_store.get_recent_memories(ticker, days=days, limit=10)

            if not memories:
                return ""

            # æŒ‰æ–¹å‘åˆ†ç±»
            bullish = [m for m in memories if m.get('direction') == 'bullish']
            bearish = [m for m in memories if m.get('direction') == 'bearish']
            neutral = [m for m in memories if m.get('direction') == 'neutral']

            # æ„å»ºæ‘˜è¦
            summary_parts = []

            if bullish:
                avg_score = sum(m.get('self_score', 5) for m in bullish) / len(bullish)
                summary_parts.append(f"å†å²çœ‹å¤šä¿¡å· {len(bullish)} æ¡ï¼ˆå¹³å‡åˆ† {avg_score:.1f}/10ï¼‰")

            if bearish:
                avg_score = sum(m.get('self_score', 5) for m in bearish) / len(bearish)
                summary_parts.append(f"å†å²çœ‹ç©ºä¿¡å· {len(bearish)} æ¡ï¼ˆå¹³å‡åˆ† {avg_score:.1f}/10ï¼‰")

            if summary_parts:
                ctx = f"ã€å†å²ä¸Šä¸‹æ–‡ã€‘{' | '.join(summary_parts)}"
                # æˆªæ–­é˜²æ­¢æ³¨å…¥è¿‡å¤§ä¸Šä¸‹æ–‡åˆ° Agent
                return ctx[:self.MAX_CONTEXT_CHARS]

            return ""

        except (ValueError, KeyError, TypeError, AttributeError) as e:
            _log.error("get_context_summary å¤±è´¥: %s", e, exc_info=True)
            return ""

    def invalidate_cache(self, ticker: Optional[str] = None) -> None:
        """
        æ¸…é™¤ç¼“å­˜

        Args:
            ticker: å¦‚ä¸º Noneï¼Œæ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼›å¦åˆ™æ¸…é™¤ç‰¹å®š ticker
        """
        with self._cache_lock:
            if ticker:
                self._cache.pop(ticker, None)
                self._tfidf_cache.pop(ticker, None)
            else:
                self._cache.clear()
                self._tfidf_cache.clear()
